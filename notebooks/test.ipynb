{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e6b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf728bd7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c17f12e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root (parent directory) to system path (for module imports)\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7574190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CocoDataset, split_coco_annotations\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RetrievalAugmentedTransformer, TransformerMappingNetwork, ImageCaptioningModel\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train, train_rat\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/notebooks/../src/dataset.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PreTrainedTokenizer\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_gpt2_tokenizer\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplit_coco_annotations\u001b[39m(\n\u001b[32m     14\u001b[39m     annotations_path: \u001b[38;5;28mstr\u001b[39m, output_dir: \u001b[38;5;28mstr\u001b[39m, split_ratio: \u001b[38;5;28mfloat\u001b[39m = \u001b[32m0.8\u001b[39m, seed: \u001b[38;5;28mint\u001b[39m = \u001b[32m42\u001b[39m\n\u001b[32m     15\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     16\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[33;03m    Split the COCO annotations JSON file into training and validation sets based on image IDs (not split by captions).\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m    When creating a `CocoDataset` instance, use the resulting JSON files (but the same `.pt` embeddings file).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m \u001b[33;03m        seed (int, optional): Random seed for reproducibility. Defaults to 42.\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/notebooks/../src/utils.py:6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPT2Tokenizer\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave_loss_curves\u001b[39m(loss_values: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m], filepath: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     10\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Saves a graph of loss values over training iterations.\u001b[39;00m\n\u001b[32m     11\u001b[39m \n\u001b[32m     12\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[33;03m        loss_values (list[float]): List of loss values recorded during training.\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m        filepath (str): The file path to save the graph image.\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/.venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/.venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.5-linux-x86_64-gnu/lib/python3.13/importlib/__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/.venv/lib/python3.13/site-packages/transformers/models/__init__.py:396\u001b[39m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m    395\u001b[39m _file = \u001b[38;5;28mglobals\u001b[39m()[\u001b[33m\"\u001b[39m\u001b[33m__file__\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m sys.modules[\u001b[34m__name__\u001b[39m] = _LazyModule(\u001b[34m__name__\u001b[39m, _file, \u001b[43mdefine_import_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_file\u001b[49m\u001b[43m)\u001b[49m, module_spec=__spec__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/.venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:2867\u001b[39m, in \u001b[36mdefine_import_structure\u001b[39m\u001b[34m(module_path, prefix)\u001b[39m\n\u001b[32m   2843\u001b[39m \u001b[38;5;129m@lru_cache\u001b[39m\n\u001b[32m   2844\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdefine_import_structure\u001b[39m(module_path: \u001b[38;5;28mstr\u001b[39m, prefix: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m) -> IMPORT_STRUCTURE_T:\n\u001b[32m   2845\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2846\u001b[39m \u001b[33;03m    This method takes a module_path as input and creates an import structure digestible by a _LazyModule.\u001b[39;00m\n\u001b[32m   2847\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2865\u001b[39m \u001b[33;03m    If `prefix` is not None, it will add that prefix to all keys in the returned dict.\u001b[39;00m\n\u001b[32m   2866\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2867\u001b[39m     import_structure = \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2868\u001b[39m     spread_dict = spread_import_structure(import_structure)\n\u001b[32m   2870\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/.venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:2580\u001b[39m, in \u001b[36mcreate_import_structure_from_path\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m   2578\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os.listdir(module_path):\n\u001b[32m   2579\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m f != \u001b[33m\"\u001b[39m\u001b[33m__pycache__\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m os.path.isdir(os.path.join(module_path, f)):\n\u001b[32m-> \u001b[39m\u001b[32m2580\u001b[39m         import_structure[f] = \u001b[43mcreate_import_structure_from_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2582\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.isdir(os.path.join(directory, f)):\n\u001b[32m   2583\u001b[39m         adjacent_modules.append(f)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/.venv/lib/python3.13/site-packages/transformers/utils/import_utils.py:2582\u001b[39m, in \u001b[36mcreate_import_structure_from_path\u001b[39m\u001b[34m(module_path)\u001b[39m\n\u001b[32m   2579\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m f != \u001b[33m\"\u001b[39m\u001b[33m__pycache__\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m os.path.isdir(os.path.join(module_path, f)):\n\u001b[32m   2580\u001b[39m         import_structure[f] = create_import_structure_from_path(os.path.join(module_path, f))\n\u001b[32m-> \u001b[39m\u001b[32m2582\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43misdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   2583\u001b[39m         adjacent_modules.append(f)\n\u001b[32m   2585\u001b[39m \u001b[38;5;66;03m# We're only taking a look at files different from __init__.py\u001b[39;00m\n\u001b[32m   2586\u001b[39m \u001b[38;5;66;03m# We could theoretically require things directly from the __init__.py\u001b[39;00m\n\u001b[32m   2587\u001b[39m \u001b[38;5;66;03m# files, but this is not supported at this time.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen genericpath>:51\u001b[39m, in \u001b[36misdir\u001b[39m\u001b[34m(s)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "from src.dataset import CocoDataset, split_coco_annotations\n",
    "from src.models import RetrievalAugmentedTransformer, TransformerMappingNetwork, ImageCaptioningModel\n",
    "from src.train import train, train_rat\n",
    "from src.database.image_store import create_objectbox_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80631cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0920e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "SEED = 42\n",
    "# TODO: Set seed for each relevant library (torch, numpy, random, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69bd4b0",
   "metadata": {},
   "source": [
    "## Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CAPTION_LENGTH = 50\n",
    "EMBEDDINGS_PATH = \"/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/data/data/coco/embeddings/\"\n",
    "ANNOTATIONS_PATH = \"/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/data/data/coco/annotations/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ddd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting: 66226 Train images, 16557 Val images.\n",
      "Created:\n",
      "- /mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/data/data/coco/annotations/train_split.json\n",
      "- /mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/data/data/coco/annotations/val_split.json\n"
     ]
    }
   ],
   "source": [
    "# We split the original COCO 2014 training set into a new training and validation set\n",
    "split_coco_annotations(\n",
    "    annotations_path=ANNOTATIONS_PATH + \"captions_train2014.json\",\n",
    "    output_dir=ANNOTATIONS_PATH,\n",
    "    split_ratio=0.8,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc1cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready: 414113 captions.\n"
     ]
    }
   ],
   "source": [
    "# Training Dataset (orig. COCO 2014 TRAIN)\n",
    "train_dataset = CocoDataset(\n",
    "    embeddings_path=EMBEDDINGS_PATH + \"train_clip_embeddings.pt\",\n",
    "    annotations_path=ANNOTATIONS_PATH + \"captions_train2014.json\",\n",
    "    max_length=MAX_CAPTION_LENGTH,\n",
    "    normalize_embeddings=False,  # `.pt` files already contain normalized embeddings\n",
    ")\n",
    "\n",
    "# # Validation Dataset (orig. COCO 2014 TRAIN)\n",
    "# val_dataset = CocoDataset(\n",
    "#     embeddings_path=EMBEDDINGS_PATH + \"train_val_clip_embeddings.pt\",\n",
    "#     annotations_path=ANNOTATIONS_PATH + \"val_split.json\",\n",
    "#     max_length=MAX_CAPTION_LENGTH,\n",
    "#     normalize_embeddings=False,\n",
    "# )\n",
    "\n",
    "# # Test Dataset (orig. COCO 2017 Val)\n",
    "# test_dataset = CocoDataset(\n",
    "#     embeddings_path=EMBEDDINGS_PATH + \"test_clip_embeddings.pt\",\n",
    "#     annotations_path=ANNOTATIONS_PATH + \"captions_val2017.json\",\n",
    "#     max_length=MAX_CAPTION_LENGTH,\n",
    "#     normalize_embeddings=False,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b93249",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b35a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/.venv/lib/python3.13/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer was not TransformerEncoderLayer\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetrievalAugmentedTransformer(\n",
      "  (mapping_network): TransformerMappingNetwork(\n",
      "    (linear): Linear(in_features=512, out_features=30720, bias=True)\n",
      "    (transformer): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-7): 8 x EncoderLayer(\n",
      "          (transformer_layer): TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "            (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.1, inplace=False)\n",
      "            (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gpt): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(50257, 768)\n",
      "      (wpe): Embedding(1024, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-11): 12 x GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D(nf=2304, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=768)\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D(nf=3072, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=3072)\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  )\n",
      "  (aggregator): RetrievalAggregator()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Models\n",
    "mapping_network = TransformerMappingNetwork(\n",
    "    embed_dim=512,  # CLIP embedding dimension\n",
    "    gpt_dim=768,  # GPT-2 embedding dimension\n",
    "    prefix_length=40,\n",
    "    hidden_length=40,\n",
    ")\n",
    "\n",
    "model = RetrievalAugmentedTransformer(\n",
    "    embed_dim=512,\n",
    "    mapping_network=mapping_network,\n",
    "    freeze_gpt_weights=True,  # We only fine-tune the mapping network during training\n",
    ").to(DEVICE)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c600890",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0ff6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only for RAT: specify ObjectBox database store path and create store.\n",
    "DB_STORE_PATH = os.path.join(os.path.expanduser(\"~\"), \"objectbox_db_fast\")\n",
    "\n",
    "db_store = create_objectbox_store(db_directory=DB_STORE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c827f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1:   0%|          | 0/6471 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Unsupported type for 'NEAREST_NEIGHBOR': <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train image captioning model\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, RetrievalAugmentedTransformer):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     \u001b[43mtrain_rat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdb_store\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_i\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m64\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, ImageCaptioningModel):\n\u001b[32m     17\u001b[39m     train(\n\u001b[32m     18\u001b[39m         train_dataset=train_dataset,\n\u001b[32m     19\u001b[39m         model=model, batch_size=\u001b[32m64\u001b[39m,\n\u001b[32m     20\u001b[39m         num_epochs=\u001b[32m1\u001b[39m,\n\u001b[32m     21\u001b[39m         device=DEVICE\n\u001b[32m     22\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/notebooks/../src/train.py:210\u001b[39m, in \u001b[36mtrain_rat\u001b[39m\u001b[34m(train_dataset, model, db_store, top_k, top_i, batch_size, num_epochs, num_workers, learning_rate, num_warmup_steps, save_every_epoch, device, outputs_dir)\u001b[39m\n\u001b[32m    205\u001b[39m optimizer.zero_grad()\n\u001b[32m    207\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# Model predicts the next token given previous tokens and image embeddings\u001b[39;00m\n\u001b[32m    209\u001b[39m \u001b[38;5;66;03m# We do not have to shift the token_ids here because the model's forward method handles that internally\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    211\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdb_store\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_i\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    213\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcaption_token_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    216\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# Compute loss and gradients\u001b[39;00m\n\u001b[32m    221\u001b[39m loss = outputs.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/notebooks/../src/models.py:630\u001b[39m, in \u001b[36mRetrievalAugmentedTransformer.forward\u001b[39m\u001b[34m(self, db_store, top_i, top_k, caption_token_ids, image_embeddings, attention_mask, labels)\u001b[39m\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[32m    628\u001b[39m     single_embedding = image_embeddings[i:i+\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m630\u001b[39m     filenames_with_scores = \u001b[43mretrieve_images_by_vector_similarity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdb_store\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_embedding_vector\u001b[49m\u001b[43m=\u001b[49m\u001b[43msingle_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_i\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_i\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    636\u001b[39m     caption_embeds = get_caption_embeddings(\n\u001b[32m    637\u001b[39m         db_store=db_store,\n\u001b[32m    638\u001b[39m         top_k=top_k,\n\u001b[32m    639\u001b[39m         filenames=[filename \u001b[38;5;28;01mfor\u001b[39;00m filename, _ \u001b[38;5;129;01min\u001b[39;00m filenames_with_scores],\n\u001b[32m    640\u001b[39m     )\n\u001b[32m    641\u001b[39m     all_retrieved_embeddings.append(caption_embeds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/notebooks/../src/database/image_store.py:110\u001b[39m, in \u001b[36mretrieve_images_by_vector_similarity\u001b[39m\u001b[34m(db_store, query_embedding_vector, top_i)\u001b[39m\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve_images_by_vector_similarity\u001b[39m(\n\u001b[32m     94\u001b[39m     db_store: Store, query_embedding_vector: np.ndarray, top_i: \u001b[38;5;28mint\u001b[39m\n\u001b[32m     95\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m     96\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[33;03m    Retrieve images via similarity search with its associated embedded caption using the query image embedding vector from the given ObjectBox store.\u001b[39;00m\n\u001b[32m     98\u001b[39m \u001b[33;03m    We do not consider similarity score that is perfectly equal to 1.0 (it means that it is the same image).\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    106\u001b[39m \u001b[33;03m        List of filenames obtained from the search\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    108\u001b[39m     query = (\n\u001b[32m    109\u001b[39m         \u001b[43mdb_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mImage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimage_embedding_vector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnearest_neighbor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embedding_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_i\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m         .build()\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    113\u001b[39m     results = query.find_with_scores()\n\u001b[32m    114\u001b[39m     \u001b[38;5;66;03m# if score is exactly 1.0, it means it's the same image, we skip it\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/.venv/lib/python3.13/site-packages/objectbox/box.py:171\u001b[39m, in \u001b[36mBox.query\u001b[39m\u001b[34m(self, condition)\u001b[39m\n\u001b[32m    169\u001b[39m qb = QueryBuilder(\u001b[38;5;28mself\u001b[39m._store, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m condition \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     \u001b[43mcondition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m qb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/.venv/lib/python3.13/site-packages/objectbox/condition.py:248\u001b[39m, in \u001b[36mPropertyQueryCondition.apply\u001b[39m\u001b[34m(self, qb)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, qb: QueryBuilder) -> obx_qb_cond:\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     c_cond = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._alias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    250\u001b[39m         qb.alias(\u001b[38;5;28mself\u001b[39m._alias)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/Users/hoxia/Documents/NLDeeznuts/gpt2-image-captioning/.venv/lib/python3.13/site-packages/objectbox/condition.py:239\u001b[39m, in \u001b[36mPropertyQueryCondition._apply_nearest_neighbor\u001b[39m\u001b[34m(self, qb)\u001b[39m\n\u001b[32m    237\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m qb.nearest_neighbors_f32(\u001b[38;5;28mself\u001b[39m._property_id, query_vector, element_count)\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported type for \u001b[39m\u001b[33m'\u001b[39m\u001b[33mNEAREST_NEIGHBOR\u001b[39m\u001b[33m'\u001b[39m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(query_vector)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: Unsupported type for 'NEAREST_NEIGHBOR': <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "# Train image captioning model\n",
    "if isinstance(model, RetrievalAugmentedTransformer):\n",
    "\n",
    "    train_rat(\n",
    "        train_dataset=train_dataset,\n",
    "        model=model,\n",
    "        db_store=db_store,\n",
    "        top_k=2,\n",
    "        top_i=4,\n",
    "        batch_size=64,\n",
    "        num_epochs=1,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "elif isinstance(model, ImageCaptioningModel):\n",
    "\n",
    "    train(\n",
    "        train_dataset=train_dataset,\n",
    "        model=model, batch_size=64,\n",
    "        num_epochs=1,\n",
    "        device=DEVICE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f4bde9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2-image-captioning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
