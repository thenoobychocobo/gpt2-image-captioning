{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7a6fa0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb9e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- Paths ---\n",
    "# Path to your trained model checkpoint (update with your Kaggle dataset path)\n",
    "MODEL_CHECKPOINT_PATH = \"/kaggle/input/your-model-dataset/best_model.pt\"\n",
    "\n",
    "# Path to your test set embeddings (update with your Kaggle dataset path)\n",
    "TEST_EMBEDDINGS_PATH = \"/kaggle/input/your-embeddings-dataset/test_embeddings.pt\"\n",
    "\n",
    "# --- Model Architecture Hyperparameters ---\n",
    "# CRITICAL: These MUST match the values used during training\n",
    "EMBED_DIM = 768         # Dimension of your image embeddings (e.g., 768 for ViT-B, 2048 for CLIP ViT-L or ResNet)\n",
    "GPT_DIM = 768           # GPT-2 embedding dimension (768 for base GPT-2, don't change unless using a different LM)\n",
    "PREFIX_LENGTH = 10      # Number of prefix tokens generated by your mapping network\n",
    "HIDDEN_LENGTH = 10      # Number of hidden tokens in your mapping network (architecture-specific)\n",
    "NUM_LAYERS = 8          # Number of transformer layers in your mapping network\n",
    "\n",
    "# --- Inference Parameters ---\n",
    "BATCH_SIZE = 64         # Batch size for inference (adjust based on available GPU memory)\n",
    "MAX_LENGTH = 50         # Maximum caption length to generate\n",
    "TEMPERATURE = 1.0       # Sampling temperature (0 = greedy, higher = more random)\n",
    "TOP_P = 0.9            # Nucleus sampling threshold\n",
    "\n",
    "# --- Output ---\n",
    "OUTPUT_FILENAME = \"results.json\"  # Default name for COCO submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1784ed9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL DEFINITIONS\n",
    "# ==============================================================================\n",
    "\n",
    "def load_gpt2_tokenizer() -> GPT2Tokenizer:\n",
    "    \"\"\"\n",
    "    Load and configure GPT-2 tokenizer.\n",
    "    \n",
    "    Returns:\n",
    "        Configured GPT2Tokenizer with pad token set to eos token\n",
    "    \"\"\"\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # GPT-2 has no dedicated pad token\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "class TransformerMappingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Maps image embeddings to a sequence of prefix tokens using a Transformer encoder.\n",
    "    \n",
    "    This network projects image embeddings into GPT-2's embedding space and processes\n",
    "    them through a transformer to generate contextual prefix tokens for caption generation.\n",
    "    \n",
    "    Args:\n",
    "        embed_dim: Dimension of input image embeddings\n",
    "        gpt_dim: Dimension of GPT-2 embeddings (typically 768)\n",
    "        prefix_length: Number of prefix tokens to generate\n",
    "        hidden_length: Number of intermediate tokens before producing final prefix\n",
    "        num_layers: Number of transformer encoder layers\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embed_dim: int,\n",
    "        gpt_dim: int,\n",
    "        prefix_length: int,\n",
    "        hidden_length: int,\n",
    "        num_layers: int = 8,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.gpt_dim = gpt_dim\n",
    "        self.hidden_length = hidden_length\n",
    "        self.prefix_length = prefix_length\n",
    "        \n",
    "        # Linear projection from image embedding space to GPT embedding space\n",
    "        self.linear = nn.Linear(embed_dim, hidden_length * gpt_dim)\n",
    "        \n",
    "        # Learnable prefix tokens that will be refined by the transformer\n",
    "        self.prefix_const = nn.Parameter(\n",
    "            torch.randn(prefix_length, gpt_dim), requires_grad=True\n",
    "        )\n",
    "        \n",
    "        # Transformer encoder to process image tokens and generate contextual prefix\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=gpt_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=int(gpt_dim * 4),\n",
    "            batch_first=True,\n",
    "            activation=\"relu\",\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass: image embeddings → prefix tokens\n",
    "        \n",
    "        Args:\n",
    "            x: Image embeddings of shape (batch_size, embed_dim)\n",
    "        \n",
    "        Returns:\n",
    "            Prefix tokens of shape (batch_size, prefix_length, gpt_dim)\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # Project and reshape: (B, E) → (B, H*G) → (B, H, G)\n",
    "        x = self.linear(x)\n",
    "        x = x.view(batch_size, self.hidden_length, self.gpt_dim)\n",
    "        \n",
    "        # Expand learnable prefix: (P, G) → (B, P, G)\n",
    "        prefix = self.prefix_const.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Concatenate image tokens + prefix tokens: (B, H+P, G)\n",
    "        inputs = torch.cat((x, prefix), dim=1)\n",
    "        \n",
    "        # Process through transformer and return only the prefix portion\n",
    "        out = self.transformer(inputs)\n",
    "        return out[:, self.hidden_length:, :]  # (B, P, G)\n",
    "\n",
    "\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Complete image captioning model combining mapping network and GPT-2.\n",
    "    \n",
    "    This model takes image embeddings, converts them to prefix tokens via the\n",
    "    mapping network, and uses GPT-2 to generate captions conditioned on these prefixes.\n",
    "    \n",
    "    Args:\n",
    "        mapping_network: Network that converts image embeddings to prefix tokens\n",
    "        image_prefix_length: Length of image prefix (defaults to mapping_network.prefix_length)\n",
    "        prefix_task_prompt: Optional task-specific text prefix (e.g., \"A photo of\")\n",
    "        tokenizer: GPT-2 tokenizer (will be loaded if not provided)\n",
    "        gpt: GPT-2 model (will be loaded if not provided)\n",
    "        freeze_gpt_weights: Whether to freeze GPT-2 weights during training\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        mapping_network: nn.Module,\n",
    "        image_prefix_length: int | None = None,\n",
    "        prefix_task_prompt: str | None = None,\n",
    "        tokenizer: GPT2Tokenizer | None = None,\n",
    "        gpt: GPT2LMHeadModel | None = None,\n",
    "        freeze_gpt_weights: bool = True,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.image_prefix_length = image_prefix_length or mapping_network.prefix_length\n",
    "        self.mapping_network = mapping_network\n",
    "        \n",
    "        # Load GPT-2 model and tokenizer\n",
    "        self.gpt = gpt or GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n",
    "        self.tokenizer = tokenizer or load_gpt2_tokenizer()\n",
    "        \n",
    "        # Freeze/unfreeze GPT-2 parameters\n",
    "        for p in self.gpt.parameters():\n",
    "            p.requires_grad = not freeze_gpt_weights\n",
    "        \n",
    "        # Optional task-specific prefix embeddings\n",
    "        self.task_prefix_embeds: nn.Parameter | None = None\n",
    "        if prefix_task_prompt:\n",
    "            with torch.no_grad():\n",
    "                task_token_ids = self.tokenizer.encode(\n",
    "                    prefix_task_prompt, return_tensors=\"pt\"\n",
    "                )\n",
    "                task_token_embeds = self.gpt.transformer.wte(task_token_ids)\n",
    "            self.task_prefix_embeds = nn.Parameter(\n",
    "                task_token_embeds.squeeze(0), requires_grad=True\n",
    "            )\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        caption_token_ids: torch.Tensor,\n",
    "        image_embeddings: torch.Tensor,\n",
    "        attention_mask: torch.Tensor | None = None,\n",
    "        labels: torch.Tensor | None = None,\n",
    "    ) -> CausalLMOutputWithCrossAttentions:\n",
    "        \"\"\"\n",
    "        Forward pass for training (computes loss on caption tokens).\n",
    "        \n",
    "        Args:\n",
    "            caption_token_ids: Tokenized captions (batch_size, seq_len)\n",
    "            image_embeddings: Image embeddings (batch_size, embed_dim)\n",
    "            attention_mask: Attention mask for caption tokens\n",
    "            labels: Target tokens for loss computation\n",
    "        \n",
    "        Returns:\n",
    "            GPT-2 model outputs including loss\n",
    "        \"\"\"\n",
    "        # Get caption token embeddings\n",
    "        caption_tokens = self.gpt.transformer.wte(caption_token_ids)\n",
    "        \n",
    "        # Generate image prefix tokens\n",
    "        prefix_tokens = self.mapping_network(image_embeddings)  # (B, P, G)\n",
    "        \n",
    "        # Add optional task prefix\n",
    "        if self.task_prefix_embeds is not None:\n",
    "            batch_size = image_embeddings.shape[0]\n",
    "            task_prefix_tokens = self.task_prefix_embeds.unsqueeze(0).expand(\n",
    "                batch_size, -1, -1\n",
    "            )\n",
    "            prefix_tokens = torch.cat((prefix_tokens, task_prefix_tokens), dim=1)\n",
    "        \n",
    "        total_prefix_length = prefix_tokens.shape[1]\n",
    "        \n",
    "        # Concatenate prefix + caption tokens\n",
    "        input_tokens = torch.cat((prefix_tokens, caption_tokens), dim=1)\n",
    "        \n",
    "        # Adjust labels to ignore prefix tokens (set to -100)\n",
    "        if labels is not None:\n",
    "            dummy_labels = torch.full(\n",
    "                (labels.shape[0], total_prefix_length),\n",
    "                -100,\n",
    "                dtype=torch.int64,\n",
    "                device=caption_token_ids.device,\n",
    "            )\n",
    "            labels = torch.cat((dummy_labels, labels), dim=1)\n",
    "        \n",
    "        # Adjust attention mask for prefix tokens\n",
    "        if attention_mask is not None:\n",
    "            dummy_attention_mask = torch.ones(\n",
    "                (attention_mask.shape[0], total_prefix_length),\n",
    "                dtype=attention_mask.dtype,\n",
    "                device=attention_mask.device,\n",
    "            )\n",
    "            attention_mask = torch.cat((dummy_attention_mask, attention_mask), dim=1)\n",
    "        \n",
    "        return self.gpt(\n",
    "            inputs_embeds=input_tokens,\n",
    "            labels=labels,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "    \n",
    "    def generate(\n",
    "        self,\n",
    "        image_embeddings: torch.Tensor,\n",
    "        max_length: int = 50,\n",
    "        temperature: float = 1.0,\n",
    "        top_p: float = 0.9,\n",
    "        stop_token_id: int = 50256,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate captions for given image embeddings using nucleus sampling.\n",
    "        \n",
    "        Args:\n",
    "            image_embeddings: Image embeddings (batch_size, embed_dim)\n",
    "            max_length: Maximum number of tokens to generate\n",
    "            temperature: Sampling temperature (0 = greedy, higher = more random)\n",
    "            top_p: Nucleus sampling threshold (keep top tokens with cumulative prob > top_p)\n",
    "            stop_token_id: Token ID to stop generation (default: GPT-2 EOS token)\n",
    "        \n",
    "        Returns:\n",
    "            Generated token IDs (batch_size, generated_length)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        device = image_embeddings.device\n",
    "        batch_size = image_embeddings.shape[0]\n",
    "        \n",
    "        # Generate prefix tokens from image embeddings\n",
    "        with torch.no_grad():\n",
    "            prefix_tokens = self.mapping_network(image_embeddings)\n",
    "        \n",
    "        # Add optional task prefix\n",
    "        if self.task_prefix_embeds is not None:\n",
    "            task_prefix_tokens = self.task_prefix_embeds.unsqueeze(0).expand(\n",
    "                batch_size, -1, -1\n",
    "            )\n",
    "            prefix_tokens = torch.cat((prefix_tokens, task_prefix_tokens), dim=1)\n",
    "        \n",
    "        current_input_tokens = prefix_tokens\n",
    "        generated_tokens: list[torch.Tensor] = []\n",
    "        is_finished = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "        \n",
    "        # Autoregressive generation loop\n",
    "        for _ in range(max_length):\n",
    "            if is_finished.all():\n",
    "                break\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # Get next token logits\n",
    "                outputs = self.gpt(inputs_embeds=current_input_tokens)\n",
    "                next_token_logits = outputs.logits[:, -1, :]\n",
    "                \n",
    "                # Apply temperature scaling\n",
    "                if temperature > 0:\n",
    "                    next_token_logits = next_token_logits / temperature\n",
    "                \n",
    "                # Apply nucleus (top-p) sampling\n",
    "                if top_p < 1.0 and temperature > 0:\n",
    "                    next_token_logits[is_finished, :] = 0.0\n",
    "                    sorted_logits, sorted_indices = torch.sort(\n",
    "                        next_token_logits, descending=True\n",
    "                    )\n",
    "                    cumulative_probs = torch.cumsum(\n",
    "                        F.softmax(sorted_logits, dim=-1), dim=-1\n",
    "                    )\n",
    "                    sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                    sorted_indices_to_remove[:, 1:] = sorted_indices_to_remove[:, :-1].clone()\n",
    "                    sorted_indices_to_remove[:, 0] = 0\n",
    "                    indices_to_remove = sorted_indices_to_remove.scatter(\n",
    "                        1, sorted_indices, sorted_indices_to_remove\n",
    "                    )\n",
    "                    next_token_logits = next_token_logits.masked_fill(\n",
    "                        indices_to_remove, float(\"-inf\")\n",
    "                    )\n",
    "                \n",
    "                # Sample next token\n",
    "                if temperature == 0:\n",
    "                    next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "                else:\n",
    "                    probs = F.softmax(next_token_logits, dim=-1)\n",
    "                    next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "                \n",
    "                # Check for stop token and mark finished sequences\n",
    "                is_current_stop = next_token_id.squeeze(-1).eq(stop_token_id)\n",
    "                is_finished = torch.logical_or(is_finished, is_current_stop)\n",
    "                next_token_id[is_finished] = 0\n",
    "                \n",
    "                generated_tokens.append(next_token_id)\n",
    "                \n",
    "                # Append to input sequence\n",
    "                next_token = self.gpt.transformer.wte(next_token_id)\n",
    "                current_input_tokens = torch.cat((current_input_tokens, next_token), dim=1)\n",
    "        \n",
    "        if not generated_tokens:\n",
    "            return torch.empty((batch_size, 0), dtype=torch.long, device=device)\n",
    "        \n",
    "        return torch.cat(generated_tokens, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b27b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    " DATASET AND DATA LOADING\n",
    "# ==============================================================================\n",
    "\n",
    "class CocoTestEmbeddingsDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset for COCO test set pre-computed embeddings.\n",
    "    \n",
    "    Expected format of embeddings file:\n",
    "        {\n",
    "            \"embeddings\": tensor of shape (N, embed_dim),\n",
    "            \"filenames\": list of N filenames (e.g., \"COCO_test2014_000000123456.jpg\")\n",
    "        }\n",
    "    \n",
    "    Args:\n",
    "        embeddings_path: Path to .pt file containing embeddings and filenames\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embeddings_path: str):\n",
    "        data = torch.load(embeddings_path, map_location=\"cpu\")\n",
    "        self.image_embeddings = data[\"embeddings\"]\n",
    "        self.image_filenames = data[\"filenames\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Extract image ID from filename (e.g., \"COCO_test2014_000000123456.jpg\" → 123456)\n",
    "        fname = self.image_filenames[idx]\n",
    "        image_id = int(fname.split(\"_\")[-1].split(\".\")[0])\n",
    "        return {\n",
    "            \"image_id\": image_id,\n",
    "            \"image_embedding\": self.image_embeddings[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Custom collate function for batching embeddings.\n",
    "    \n",
    "    Args:\n",
    "        batch: List of dictionaries from dataset\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with batched tensors\n",
    "    \"\"\"\n",
    "    ids = [b[\"image_id\"] for b in batch]\n",
    "    embs = torch.stack([b[\"image_embedding\"] for b in batch], dim=0)\n",
    "    return {\n",
    "        \"image_id\": torch.tensor(ids, dtype=torch.long),\n",
    "        \"image_embedding\": embs\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92dce70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL SETUP AND CHECKPOINT LOADING\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize tokenizer\n",
    "gpt2_tokenizer = load_gpt2_tokenizer()\n",
    "print(f\"✓ Loaded GPT-2 tokenizer\")\n",
    "\n",
    "# Build mapping network with your hyperparameters\n",
    "mapping_network = TransformerMappingNetwork(\n",
    "    embed_dim=EMBED_DIM,\n",
    "    gpt_dim=GPT_DIM,\n",
    "    prefix_length=PREFIX_LENGTH,\n",
    "    hidden_length=HIDDEN_LENGTH,\n",
    "    num_layers=NUM_LAYERS,\n",
    ")\n",
    "print(f\"✓ Built TransformerMappingNetwork (embed_dim={EMBED_DIM}, prefix_length={PREFIX_LENGTH})\")\n",
    "\n",
    "# Build complete captioning model\n",
    "model = ImageCaptioningModel(\n",
    "    mapping_network=mapping_network,\n",
    "    tokenizer=gpt2_tokenizer,\n",
    "    freeze_gpt_weights=True,  # GPT-2 weights are frozen during inference\n",
    ").to(DEVICE)\n",
    "print(f\"✓ Built ImageCaptioningModel\")\n",
    "\n",
    "# Load trained weights from checkpoint\n",
    "print(f\"\\nLoading checkpoint from: {MODEL_CHECKPOINT_PATH}\")\n",
    "state_dict = torch.load(MODEL_CHECKPOINT_PATH, map_location=\"cpu\")\n",
    "\n",
    "# Extract only mapping_network weights and strip the \"mapping_network.\" prefix\n",
    "# (Checkpoint keys are like \"mapping_network.linear.weight\", but we need \"linear.weight\")\n",
    "mapping_state = {\n",
    "    k.split(\"mapping_network.\", 1)[1]: v\n",
    "    for k, v in state_dict.items()\n",
    "    if k.startswith(\"mapping_network.\")\n",
    "}\n",
    "\n",
    "# Load the weights into the mapping network\n",
    "model.mapping_network.load_state_dict(mapping_state)\n",
    "model.eval()\n",
    "print(f\"✓ Loaded mapping network weights successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd7a4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST DATASET SETUP\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING TEST DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "test_dataset = CocoTestEmbeddingsDataset(TEST_EMBEDDINGS_PATH)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,  # Set to 0 to avoid multiprocessing issues with GPU tensors\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "print(f\"✓ Loaded test dataset: {len(test_dataset)} images\")\n",
    "print(f\"✓ Batch size: {BATCH_SIZE}\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# CAPTION GENERATION\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING CAPTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "predictions: list[dict] = []\n",
    "seen: set[int] = set()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc=\"Generating captions\", unit=\"batch\"):\n",
    "        image_ids = batch[\"image_id\"]\n",
    "        image_embeddings = batch[\"image_embedding\"].to(DEVICE)\n",
    "        \n",
    "        # Generate captions\n",
    "        generated_ids = model.generate(\n",
    "            image_embeddings=image_embeddings,\n",
    "            max_length=MAX_LENGTH,\n",
    "            temperature=TEMPERATURE,\n",
    "            top_p=TOP_P,\n",
    "        )\n",
    "        \n",
    "        # Decode to text\n",
    "        captions = gpt2_tokenizer.batch_decode(\n",
    "            generated_ids, skip_special_tokens=True\n",
    "        )\n",
    "        \n",
    "        # Add to predictions list (ensure one caption per image)\n",
    "        for img_id, cap in zip(image_ids, captions):\n",
    "            img_id_int = int(img_id.item())\n",
    "            if img_id_int not in seen:\n",
    "                predictions.append({\"image_id\": img_id_int, \"caption\": cap})\n",
    "                seen.add(img_id_int)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SAVE RESULTS\n",
    "# ==============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "with open(OUTPUT_FILENAME, \"w\") as f:\n",
    "    json.dump(predictions, f)\n",
    "\n",
    "print(f\"✓ Saved {len(predictions)} predictions to {OUTPUT_FILENAME}\")\n",
    "print(f\"✓ Ready for submission to COCO evaluation server!\")\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Download the results.json file from Kaggle\")\n",
    "print(\"2. Submit to: https://competitions.codalab.org/competitions/3221\")\n",
    "print(\"3. Check the 'Participate > Submit / View Results' tab\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
