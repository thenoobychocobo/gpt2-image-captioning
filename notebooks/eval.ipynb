{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move up to project root directory (parent directory) for module imports\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "# Current working directory should now be project root\n",
    "print(\"Current working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from transformers import set_seed\n",
    "import tempfile\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src.visualize import create_captioning_dataset\n",
    "\n",
    "from src.dataset import CocoDataset\n",
    "from src.models import ImageCaptioningModel, TransformerMappingNetwork\n",
    "from src.train import train\n",
    "\n",
    "from src.eval import compute_caption_metrics, evaluate_captions, evaluate_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "SEED = 42\n",
    "set_seed(SEED) # Helper function that sets the seed in all relevant libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prepare Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CAPTION_LENGTH = 50\n",
    "DATA_DIR = \"coco_data/\"\n",
    "EMBEDDINGS_PATH = DATA_DIR + \"embeddings/\"\n",
    "ANNOTATIONS_PATH = DATA_DIR + \"annotations/\"\n",
    "CHECKPOINTS_PATH = \"checkpoints/\"\n",
    "VAL_PATH = DATA_DIR + \"val2017/\"  # For FiftyOne visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update annotation paths\n",
    "\n",
    "# Training Dataset\n",
    "train_dataset = CocoDataset(\n",
    "    embeddings_path=EMBEDDINGS_PATH + \"train_clip_embeddings.pt\",\n",
    "    annotations_path=ANNOTATIONS_PATH + \"captions_train2017.json\", \n",
    "    max_length=MAX_CAPTION_LENGTH,\n",
    "    normalize_embeddings=False,  # `.pt` files already contain normalized embeddings\n",
    ")\n",
    "\n",
    "# Validation Dataset\n",
    "val_dataset = CocoDataset(\n",
    "    embeddings_path=EMBEDDINGS_PATH + \"val_clip_embeddings.pt\",\n",
    "    annotations_path=ANNOTATIONS_PATH + \"captions_val2017.json\",\n",
    "    max_length=MAX_CAPTION_LENGTH,\n",
    "    normalize_embeddings=False,\n",
    ")\n",
    "\n",
    "# Note: No test dataset as "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prepare Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "mapping_network = TransformerMappingNetwork(\n",
    "    embed_dim=512,  # Embedding dimension\n",
    "    gpt_dim=768,  # GPT-2 embedding dimension\n",
    "    prefix_length=40,\n",
    "    hidden_length=40,\n",
    ")\n",
    "\n",
    "model = ImageCaptioningModel(\n",
    "    mapping_network=mapping_network,\n",
    "    freeze_gpt_weights=True,  # We only fine-tune the mapping network during training\n",
    ").to(DEVICE)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Train Model (with Validation Evaluation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train image captioning model with validation evaluation\n",
    "history = train(\n",
    "    train_dataset=train_dataset,\n",
    "    model=model,\n",
    "    batch_size=64,\n",
    "    num_epochs=1,\n",
    "    device=DEVICE,\n",
    "    outputs_dir=CHECKPOINTS_PATH,\n",
    "    # Evaluation on validation set\n",
    "    val_dataset=val_dataset,\n",
    "    val_annotations_path=ANNOTATIONS_PATH + \"val_split.json\",\n",
    "    eval_every_epoch=1,\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"\\nBest validation CIDEr: {history['best_val_cider']:.4f} at epoch {history['best_epoch']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "Run evaluation on the test set after training is complete.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model checkpoint (optional - if you want to evaluate a saved model)\n",
    "# checkpoint_path = CHECKPOINTS_PATH + \"best_model.pth\"\n",
    "# model.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n",
    "# print(f\"Loaded checkpoint: {checkpoint_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on TEST set (final evaluation - only run once after training!)\n",
    "test_metrics = evaluate_epoch(\n",
    "    model=model,\n",
    "    dataset=test_dataset,\n",
    "    annotations_path=ANNOTATIONS_PATH + \"captions_val2017.json\",\n",
    "    epoch=history[\"best_epoch\"],\n",
    "    split_name=\"test\",\n",
    "    batch_size=32,\n",
    "    num_workers=4,  # Set to 0 on Windows\n",
    "    device=DEVICE,\n",
    "    output_dir=CHECKPOINTS_PATH + \"eval_results\",\n",
    ")\n",
    "\n",
    "print(f\"\\nFinal Test Results: {test_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_path = (\n",
    "    CHECKPOINTS_PATH\n",
    "    + f\"eval_results/epoch_{history['best_epoch']}_test_predictions.json\"\n",
    ")\n",
    "with open(predictions_path) as f:\n",
    "    predictions = json.load(f)\n",
    "\n",
    "print(f\"Generated {len(predictions)} captions\\n\")\n",
    "print(\"Sample predictions:\")\n",
    "for pred in predictions[:10]:\n",
    "    print(f\"  Image {pred['image_id']}: {pred['caption']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Test: Evaluation Functions with Mock Data\n",
    "\n",
    "Test the evaluation functions without needing real data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test compute_caption_metrics with mock data\n",
    "print(\"Testing compute_caption_metrics()...\")\n",
    "\n",
    "mock_predictions = {\n",
    "    1: [\"a dog sitting on the grass\"],\n",
    "    2: [\"a cat sleeping on a couch\"],\n",
    "    3: [\"a person riding a bicycle\"],\n",
    "}\n",
    "\n",
    "mock_references = {\n",
    "    1: [\n",
    "        \"a dog is sitting on green grass\",\n",
    "        \"dog on the grass\",\n",
    "        \"a brown dog sits on grass\",\n",
    "    ],\n",
    "    2: [\n",
    "        \"a cat is sleeping on the sofa\",\n",
    "        \"cat napping on couch\",\n",
    "        \"a sleeping cat on a couch\",\n",
    "    ],\n",
    "    3: [\"a man rides a bike\", \"person on a bicycle\", \"someone cycling on the road\"],\n",
    "}\n",
    "\n",
    "metrics = compute_caption_metrics(mock_predictions, mock_references)\n",
    "print(f\"✅ Success! Metrics: {metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test evaluate_captions with mock COCO format\n",
    "print(\"Testing evaluate_captions()...\")\n",
    "\n",
    "mock_coco = {\n",
    "    \"images\": [\n",
    "        {\"id\": 1, \"file_name\": \"img1.jpg\"},\n",
    "        {\"id\": 2, \"file_name\": \"img2.jpg\"},\n",
    "    ],\n",
    "    \"annotations\": [\n",
    "        {\"image_id\": 1, \"id\": 1, \"caption\": \"a dog sitting on grass\"},\n",
    "        {\"image_id\": 1, \"id\": 2, \"caption\": \"dog on the green grass\"},\n",
    "        {\"image_id\": 2, \"id\": 3, \"caption\": \"a cat on a couch\"},\n",
    "        {\"image_id\": 2, \"id\": 4, \"caption\": \"cat sleeping on sofa\"},\n",
    "    ],\n",
    "}\n",
    "\n",
    "mock_pred_list = [\n",
    "    {\"image_id\": 1, \"caption\": \"a dog sitting on the grass\"},\n",
    "    {\"image_id\": 2, \"caption\": \"a cat sleeping on the couch\"},\n",
    "]\n",
    "\n",
    "with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".json\", delete=False) as f:\n",
    "    json.dump(mock_coco, f)\n",
    "    temp_path = f.name\n",
    "\n",
    "try:\n",
    "    metrics = evaluate_captions(mock_pred_list, temp_path)\n",
    "    print(f\"✅ Success! Metrics: {metrics}\")\n",
    "finally:\n",
    "    os.unlink(temp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## FiftyOne Visualization\n",
    "\n",
    "Interactive visualization of generated captions vs. ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create FiftyOne dataset from evaluation results\n",
    "dataset = create_captioning_dataset(\n",
    "    images_dir=VAL_PATH,\n",
    "    predictions_path=CHECKPOINTS_PATH\n",
    "    + f\"eval_results/epoch_{history['best_epoch']}_test_predictions.json\",\n",
    "    annotations_path=ANNOTATIONS_PATH + \"captions_val2017.json\",\n",
    "    dataset_name=\"caption_eval\",\n",
    ")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Browse samples in the dataset\n",
    "for sample in dataset.take(5):\n",
    "    print(f\"\\nImage ID: {sample.image_id}\")\n",
    "    print(f\"Generated: {sample.generated_caption}\")\n",
    "    print(f\"References: {sample.reference_captions[:2]}...\")  # Show first 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch FiftyOne app for interactive exploration\n",
    "# This opens a web browser at http://localhost:5151\n",
    "\n",
    "# Uncomment to launch:\n",
    "# launch_app(dataset)\n",
    "\n",
    "# Or launch without blocking:\n",
    "# session = fo.launch_app(dataset)\n",
    "# session.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Plot Metrics Over Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot validation metrics from training history (if available)\n",
    "if history[\"val_metrics\"]:\n",
    "    val_history = history[\"val_metrics\"]\n",
    "    epochs = [m[\"epoch\"] for m in val_history]\n",
    "\n",
    "    # Extract metrics\n",
    "    bleu4 = [m.get(\"BLEU-4\", 0) for m in val_history]\n",
    "    cider = [m.get(\"CIDEr\", 0) for m in val_history]\n",
    "    meteor = [m.get(\"METEOR\", 0) for m in val_history]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.plot(epochs, bleu4, \"b-o\", label=\"BLEU-4\")\n",
    "    ax.plot(epochs, cider, \"r-s\", label=\"CIDEr\")\n",
    "    ax.plot(epochs, meteor, \"g-^\", label=\"METEOR\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Score\")\n",
    "    ax.set_title(\"Validation Metrics Over Training\")\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No validation metrics available - train for more epochs to see trends\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "### Evaluation Functions:\n",
    "- `compute_caption_metrics(preds, refs)` - Low-level metric computation\n",
    "- `evaluate_captions(predictions, annotations_path)` - Evaluate list of predictions  \n",
    "- `generate_and_evaluate(model, dataset, ...)` - Generate + evaluate in one call\n",
    "- `evaluate_epoch(model, dataset, ...)` - Full epoch eval with file saving\n",
    "\n",
    "### Visualization Functions:\n",
    "- `create_captioning_dataset(...)` - Build FiftyOne dataset\n",
    "- `create_comparison_dataset(...)` - Compare multiple models\n",
    "- `get_low_score_view(...)` / `get_high_score_view(...)` - Filter samples\n",
    "- `launch_app(dataset)` - Interactive web visualization\n",
    "\n",
    "### Metrics:\n",
    "- **BLEU-1/2/3/4**: N-gram precision\n",
    "- **METEOR**: Semantic matching with synonyms\n",
    "- **CIDEr**: Consensus-based TF-IDF weighted (most important for captioning)\n",
    "- **ROUGE-L**: Longest common subsequence\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2-image-captioning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
