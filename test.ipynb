{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52e6b53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7574190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.dataset import CocoDataset\n",
    "from src.models import ClipCapModel, MLPMappingNetwork\n",
    "from src.train import train\n",
    "from src.test import generate_test_caption_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "041305e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready: 414113 captions for 82783 images.\n"
     ]
    }
   ],
   "source": [
    "# Take note that the embeddings in coco_train2014_image_embeddings.pt are already normalised\n",
    "\n",
    "dataset = CocoDataset(\n",
    "    embeddings_path=\"data/coco_train2014_image_embeddings.pt\",\n",
    "    annotations_path=\"data/coco_train2014_captions.json\",\n",
    "    max_length=50,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=4, \n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "87bb1782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in dataloader: 103529\n",
      "Batch contents:\n",
      "    token_ids shape: torch.Size([4, 50])\n",
      "    clip_embedding shape: torch.Size([4, 512])\n",
      "    attention_mask shape: torch.Size([4, 50])\n",
      "    captions: ['A man riding some skis down a snowy trail.', 'a young boy is riding on a board outside', 'A man riding skis down the side of a snow covered ski slope.', 'Man holding a cat that is wearing a costume.']\n",
      "    image ids: tensor([143306, 493888, 538187, 478675])\n"
     ]
    }
   ],
   "source": [
    "# Number of batches\n",
    "print(f\"Number of batches in dataloader: {len(dataloader)}\")\n",
    "\n",
    "# View one batch\n",
    "for batch in dataloader:\n",
    "    print(\n",
    "        f\"Batch contents:\"\n",
    "        f\"\\n    token_ids shape: {batch['token_ids'].shape}\"\n",
    "        f\"\\n    clip_embedding shape: {batch['clip_embedding'].shape}\"\n",
    "        f\"\\n    attention_mask shape: {batch['attention_mask'].shape}\"\n",
    "        f\"\\n    captions: {batch['caption_text']}\"\n",
    "        f\"\\n    image ids: {batch['image_id']}\"\n",
    "    )\n",
    "    # Dataloader is smart: will stack non-tensor items as well\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c87b35a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Models\n",
    "mapping_network = MLPMappingNetwork(prefix_length=10)\n",
    "model = ClipCapModel(mapping_network=mapping_network).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "485553f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClipCapModel(\n",
      "  (mapping_network): MLPMappingNetwork(\n",
      "    (model): Sequential(\n",
      "      (0): Linear(in_features=512, out_features=3840, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=3840, out_features=7680, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (gpt): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(50257, 768)\n",
      "      (wpe): Embedding(1024, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-11): 12 x GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D(nf=2304, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=768)\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D(nf=3072, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=3072)\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c827f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train image captioning model\n",
    "train(\n",
    "    train_dataset=dataset, \n",
    "    model=model,\n",
    "    batch_size=4, \n",
    "    num_epochs=1, \n",
    "    device=device\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2-image-captioning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
