{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e6b53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf728bd7",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7574190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from src.dataset import CocoDataset, split_coco_annotations\n",
    "from src.models import ImageCaptioningModel, TransformerMappingNetwork\n",
    "from src.train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d80631cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0920e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "SEED = 42\n",
    "# TODO: Set seed for each relevant library (torch, numpy, random, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69bd4b0",
   "metadata": {},
   "source": [
    "## Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cefd503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_CAPTION_LENGTH = 50\n",
    "EMBEDDINGS_PATH = \"data/coco/embeddings/\"\n",
    "ANNOTATIONS_PATH = \"data/coco/annotations/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06ddd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting: 66226 Train images, 16557 Val images.\n",
      "Created:\n",
      "- data/coco/annotations/train_split.json\n",
      "- data/coco/annotations/val_split.json\n"
     ]
    }
   ],
   "source": [
    "# We split the original COCO 2014 training set into a new training and validation set\n",
    "split_coco_annotations(\n",
    "    annotations_path=ANNOTATIONS_PATH + \"captions_train2014.json\",\n",
    "    output_dir=ANNOTATIONS_PATH,\n",
    "    split_ratio=0.8,\n",
    "    seed=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68cc1cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ready: 331287 captions.\n",
      "Dataset ready: 82826 captions.\n",
      "Dataset ready: 25014 captions.\n"
     ]
    }
   ],
   "source": [
    "# Training Dataset (orig. COCO 2014 TRAIN)\n",
    "train_dataset = CocoDataset(\n",
    "    embeddings_path=EMBEDDINGS_PATH + \"train_val_clip_embeddings.pt\",\n",
    "    annotations_path=ANNOTATIONS_PATH + \"train_split.json\",\n",
    "    max_length=MAX_CAPTION_LENGTH,\n",
    "    normalize_embeddings=False,  # `.pt` files already contain normalized embeddings\n",
    ")\n",
    "\n",
    "# Validation Dataset (orig. COCO 2014 TRAIN)\n",
    "val_dataset = CocoDataset(\n",
    "    embeddings_path=EMBEDDINGS_PATH + \"train_val_clip_embeddings.pt\",\n",
    "    annotations_path=ANNOTATIONS_PATH + \"val_split.json\",\n",
    "    max_length=MAX_CAPTION_LENGTH,\n",
    "    normalize_embeddings=False,\n",
    ")\n",
    "\n",
    "# Test Dataset (orig. COCO 2017 Val)\n",
    "test_dataset = CocoDataset(\n",
    "    embeddings_path=EMBEDDINGS_PATH + \"test_clip_embeddings.pt\",\n",
    "    annotations_path=ANNOTATIONS_PATH + \"captions_val2017.json\",\n",
    "    max_length=MAX_CAPTION_LENGTH,\n",
    "    normalize_embeddings=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b93249",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87b35a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ryan Lee\\Desktop\\50.040 Natural Language Processing\\gpt2-image-captioning\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageCaptioningModel(\n",
      "  (mapping_network): TransformerMappingNetwork(\n",
      "    (linear): Linear(in_features=512, out_features=30720, bias=True)\n",
      "    (transformer): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-7): 8 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (gpt): GPT2LMHeadModel(\n",
      "    (transformer): GPT2Model(\n",
      "      (wte): Embedding(50257, 768)\n",
      "      (wpe): Embedding(1024, 768)\n",
      "      (drop): Dropout(p=0.1, inplace=False)\n",
      "      (h): ModuleList(\n",
      "        (0-11): 12 x GPT2Block(\n",
      "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): GPT2Attention(\n",
      "            (c_attn): Conv1D(nf=2304, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=768)\n",
      "            (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "            (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): GPT2MLP(\n",
      "            (c_fc): Conv1D(nf=3072, nx=768)\n",
      "            (c_proj): Conv1D(nf=768, nx=3072)\n",
      "            (act): NewGELUActivation()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Models\n",
    "mapping_network = TransformerMappingNetwork(\n",
    "    embed_dim=512,  # CLIP embedding dimension\n",
    "    gpt_dim=768,  # GPT-2 embedding dimension\n",
    "    prefix_length=40,\n",
    "    hidden_length=40,\n",
    ")\n",
    "\n",
    "model = ImageCaptioningModel(\n",
    "    mapping_network=mapping_network,\n",
    "    freeze_gpt_weights=True,  # We only fine-tune the mapping network during training\n",
    ").to(DEVICE)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c600890",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c827f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train image captioning model\n",
    "train(\n",
    "    train_dataset=train_dataset, model=model, batch_size=64, num_epochs=1, device=DEVICE\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt2-image-captioning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
